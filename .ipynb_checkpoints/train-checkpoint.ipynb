{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "import pickle\n",
    "from time import time\n",
    "import dataset\n",
    "\n",
    "def init_normal(shape, name=None):\n",
    "    return initializers.normal(shape)\n",
    "\n",
    "def get_Model(num_users, num_items, latent_dim, user_con_len, item_con_len, layers = [20,10,5], regs=[0,0,0]):\n",
    "\t# Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    user_embedding = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                  embeddings_initializer='uniform', W_regularizer = l2(regs[0]), input_length=1)\n",
    "    item_embedding = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                  embeddings_initializer='uniform', W_regularizer = l2(regs[1]), input_length=1)   \n",
    "    \n",
    "    user_latent = Flatten()(user_embedding(user_input))\n",
    "    item_latent = Flatten()(item_embedding(item_input))\n",
    "\n",
    "    vector = merge([user_latent, item_latent], mode = 'concat')\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        hidden = Dense(layers[i], activation='relu', init='lecun_uniform', name='ui_hidden_' + str(i))\n",
    "        vector = hidden(vector)\n",
    "\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = 'prediction')(vector)\n",
    "    \n",
    "    user_context = Dense(user_con_len, activation='sigmoid', init='lecun_uniform', name='user_context')(user_latent)\n",
    "    item_context = Dense(item_con_len, activation='sigmoid', init='lecun_uniform', name='item_context')(item_latent)\n",
    "        \n",
    "    model = Model(input=[user_input, item_input], output=[prediction, user_context, item_context])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_Model(100000, 100000, 10, 37002, 12223)\n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_instances(train_data):\n",
    "    while 1:\n",
    "        user_input = train_data['user_input']\n",
    "        item_input = train_data['item_input']\n",
    "        ui_label = train_data['ui_label']\n",
    "        u_context = train_data['u_context']\n",
    "        s_context = train_data['s_context']\n",
    "        for i in range(len(u_context)):\n",
    "            u = []\n",
    "            it = []\n",
    "            p = []\n",
    "            u.append(user_input[i])\n",
    "            it.append(item_input[i])\n",
    "            p.append(ui_label[i])\n",
    "            x = {'user_input':np.array(u), 'item_input':np.array(it)}\n",
    "            y = {'prediction':np.array(p), 'user_context':np.array(u_context[i]).reshape((1, 37002)), 'item_context':np.array(s_context[i]).reshape((1, 12223))}\n",
    "            yield (x, y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = None\n",
    "with open('data/traindata_small.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_input = train['user']\n",
    "item_input = train['spot']\n",
    "ui_label = train['label']\n",
    "data = dataset.Dataset('_small')\n",
    "data.generateContextLabels()\n",
    "contexts = data.context_data\n",
    "u_context, s_context = contexts['user_context'], contexts['spot_context']\n",
    "train_data = {}\n",
    "train_data['user_input'] = user_input\n",
    "train_data['item_input'] = item_input\n",
    "train_data['ui_label'] = ui_label\n",
    "train_data['u_context'] = u_context\n",
    "train_data['s_context'] = s_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    layers = eval(\"[16,8]\")\n",
    "    reg_layers = eval(\"[0,0]\")\n",
    "    learner = \"Adam\"\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 100\n",
    "    batch_size = 1024\n",
    "    verbose = 1\n",
    "    losses = ['binary_crossentropy','categorical_crossentropy', 'categorical_crossentropy']\n",
    "\n",
    "    num_users, num_items = len(user_input), len(item_input)\n",
    "    num_user_context = len(u_context[0])\n",
    "    num_item_context = len(s_context[0])\n",
    "\n",
    "    print('Build model')\n",
    "    class LossHistory(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "            self.accs = []\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.accs.append(logs.get('acc'))\n",
    "\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "    board = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0)  \n",
    "    \n",
    "    history = LossHistory()\n",
    "    model = get_Model(num_users, num_items, 10, 37002, 12223, layers, reg_layers)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=losses, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    print('Start Training')\n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        t1 = time()\n",
    "        hist = model.fit_generator(get_train_instances(train_data), samples_per_epoch=batch_size, nb_epoch=10, verbose=1, callbacks=[history,board])\n",
    "        t2 = time()\n",
    "        print(epoch, t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
